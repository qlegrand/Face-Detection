# -*- coding: utf-8 -*-
"""FaceNetUtils

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1avEU89DXjqWqNNJHY-HyfUchCmTJ2asn
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import keras
import os
from keras import layers, activations, models, optimizers, utils, regularizers, initializers
from keras.callbacks import Callback
import keras.backend as K
from sklearn.metrics import accuracy_score, log_loss
from mxnet import recordio
import mxnet as mx
import imageio
import sys
import warnings
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import zipfile

if sys.version_info >= (3, 0, 0):
    import urllib.request as urllib # ugly but works
else:
    import urllib

#------------------------------------------------
#
#                   LR_finder
#
#------------------------------------------------

class LR_finder(Callback):
    def __init__(self, min_lr=1.0e-8, max_lr=1.0, method="exp", plot_lr=True, max_y = 3.0):
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.method = method
        self.max_y = max_y
        self.plot_lr = plot_lr
        warnings.filterwarnings("ignore")
        
    def on_train_begin(self, logs={}):
        if self.params['steps'] != None:
            self.total_steps = self.params['steps']*self.params['epochs']
        else:
            self.total_steps = np.ceil(self.params['samples'] / self.params['batch_size']) * self.params['epochs']
        if self.method == "exp":
            self.base = np.power((self.max_lr/self.min_lr),1/(self.total_steps -1))
        else:
            self.base = (self.max_lr - self.min_lr)/(self.total_steps -1)
        self.lr_sched = self.min_lr
        self.losses = []
        self.lr_list = []
        self.val_acc_05 = []
        self.val_losses = []
        return
    
    def on_batch_begin(self, batch, logs={}):
        K.set_value(self.model.optimizer.lr, self.lr_sched)
        return
        
    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs['loss'])
        self.lr_list.append(self.lr_sched)
        val_predict = (np.asarray(self.model.predict(self.validation_data[0])))
        val_targ = self.validation_data[1]
        self.val_acc_05.append(accuracy_score(np.squeeze(val_targ), np.squeeze(val_predict.round())))
        self.val_losses.append(log_loss(np.squeeze(val_targ), np.squeeze(val_predict)))
        if self.method=="exp":
            self.lr_sched *= self.base
        else:
            self.lr_sched += self.base
        return
      
    def on_train_end(self, epoch, logs={}):
        if self.plot_lr:
            self.plot_lr_curve()
            self.plot_lr_search(alpha=0.99, scale="lin", max_y=self.max_y, metric="losses")
            self.plot_lr_search(alpha=0.99, scale="lin", metric="val_acc")
            self.plot_lr_search(alpha=0.99, scale="log", max_y=self.max_y, metric="losses")
            self.plot_lr_search(alpha=0.99, scale="log", metric="val_acc")
    
    def plot_lr_curve(self):
        plt.figure(figsize = (10, 6))
        plt.plot(self.lr_list)
        plt.xlabel("steps")
        plt.ylabel("lr")
        plt.show()
    
    def __ewma_vectorized(self, data, alpha):
        alpha_ = 1-alpha
        D = pd.DataFrame(data)
        M = D.ewm(alpha=alpha_).mean()
        out = M.values
        return out
    
    def plot_lr_search(self, alpha=0.99, scale="log", max_y=None, metric="losses"):
        metrics_to_evaluate = {'losses':[self.losses,"losses",self.val_losses], 'val_acc':[self.val_acc_05,"validation accuracy"]}
        plot_y = self.__ewma_vectorized(metrics_to_evaluate.get(metric, self.losses)[0], alpha)                    
        plt.figure(figsize = (10, 6))
        if scale=="log":
            plt.xscale("log")
            plt.xlabel("log lr")
        else:
            plt.xlabel("lr")                              
        plt.plot(self.lr_list, plot_y)
        if metric=="losses":
            plot_y2 = self.__ewma_vectorized(metrics_to_evaluate.get(metric, self.val_losses)[2], alpha)
            plt.plot(self.lr_list, plot_y2, 'g')
        if max_y != None and min(plot_y) < max_y and min(plot_y2) < max_y: plt.ylim((0.0,max_y))
        plt.ylabel(metrics_to_evaluate.get(metric, self.losses)[1])
        plt.show()

#------------------------------------------------
#
#                   LR_scheduler
#
#------------------------------------------------
        
class LR_scheduler(Callback):
    def __init__(self, min_lr, scale_factor=1.0, period=1, schedule="triangle", plot_schedule=True): 
        self.scale_factor = scale_factor
        self.min_lr = min_lr
        self.period =  period
        self.schedule = schedule
        self.plot_schedule = plot_schedule
        
    def on_train_begin(self, logs={}):
        if self.params['steps'] != None:
            self.steps_in_epoch = self.params['steps']
        else:
            self.steps_in_epoch = np.ceil(self.params['samples'] / self.params['batch_size'])
        self.step = 0
        self.nb_steps = 0
        self.amplitude_lr = 0
        self.slope_lr = 0
        self.max_lr = K.get_value(self.model.optimizer.lr)
        self.saved_lr = K.get_value(self.model.optimizer.lr)
        
        if self.schedule == "triangle":
            self.steps_in_period = self.period*self.steps_in_epoch
            self.amplitude_lr = (self.max_lr - self.min_lr)
        else:
            self.frequency = np.pi/(self.period*self.steps_in_epoch)
            self.amplitude_lr = (self.max_lr - self.min_lr)/2
            
        self.lr_stack = []
        return
    
    def on_epoch_end(self, epoch, logs={}):
        epoch_ = epoch + 1
        if epoch_ % self.period == 0:     
            self.step = 0
            self.amplitude_lr *= self.scale_factor
        return
    
    def on_train_end(self, epoch, logs={}):    
        K.set_value(self.model.optimizer.lr, self.saved_lr)
        if self.plot_schedule: self.plot_lr_schedule()
        return
    
    def on_batch_begin(self, batch, logs={}):
        if self.schedule == "triangle":
            x = abs(- self.amplitude_lr + 2 * self.amplitude_lr/self.steps_in_period * self.step)
            learning_rate = self.min_lr + max(0, self.amplitude_lr - x)
        else:
            learning_rate = self.amplitude_lr * np.cos(self.step * self.frequency) + self.amplitude_lr + self.min_lr
        K.set_value(self.model.optimizer.lr, learning_rate)
        self.lr_stack.append(learning_rate)
        self.nb_steps+=1
        self.step+=1
        return

    def plot_lr_schedule(self):
        plt.figure(figsize = (10, 6))
        plt.plot(np.arange(self.nb_steps), self.lr_stack)
        plt.xlabel("#steps")
        plt.ylabel("lr schedule")

#------------------------------------------------
#
#                   TrainLogger
#
#------------------------------------------------

class TrainLogger(Callback):
    def __init__(self, train_parameters, logs_name, logs_dir='', alpha=0.99, max_y=None, plot_log=True):
        self.alpha = alpha
        self.max_y = max_y
        self.plot_log = plot_log
        self.train_parameters = train_parameters

        steps_logs_dir = ''
        epochs_logs_dir = ''
        self.logs_dir = logs_dir
        self.steps_logs_dir = os.path.join(logs_dir,'steps_logs/')
        self.epochs_logs_dir = os.path.join(logs_dir,'epochs_logs/')
        self.models_logs_dir = os.path.join(logs_dir,'models_logs/')
        
        if not os.path.exists(self.steps_logs_dir):
            os.makedirs(self.steps_logs_dir)
        if not os.path.exists(self.epochs_logs_dir):
            os.makedirs(self.epochs_logs_dir)
        if not os.path.exists(self.models_logs_dir):
            os.makedirs(self.models_logs_dir)
            
        self.steps_logs_file = os.path.join(self.steps_logs_dir,logs_name)
        self.epochs_logs_file = os.path.join(self.epochs_logs_dir,logs_name)
        self.models_logs_file = os.path.join(self.models_logs_dir,logs_name)
 
    def on_train_begin(self, logs={}):
        self.train_loss = []
        self.train_acc = []
        self.e_train_loss = []
        self.e_train_acc = []
        self.e_val_loss = []
        self.e_val_acc = []
        self.e_steps = []
        self.nb_steps = 0
        return

    def on_batch_end(self, batch, logs={}):
        self.train_loss.append(logs['loss'])
        self.train_acc.append(logs['acc'])
        self.nb_steps+=1

        return
      
    def __ewma_vectorized(self, data, alpha):
        alpha_ = 1-alpha
        D = pd.DataFrame(data)
        M = D.ewm(alpha=alpha_).mean()
        out = M.values
        return out
      
    def on_epoch_end(self, epoch, logs={}):
        self.e_train_loss.append(logs['loss'])
        self.e_val_loss.append(logs['val_loss'])
        self.e_train_acc.append(logs['acc'])
        self.e_val_acc.append(logs['val_acc'])
        self.e_steps.append(epoch)

        return
    
    def on_train_end(self, epoch, logs={}):
        self.train_parameters['steps'] = self.nb_steps
        if self.plot_log: self.plot_history(alpha=self.alpha,max_y=self.max_y)  
        self.__save_logs(self.steps_logs_file,train_loss=self.train_loss,train_acc=self.train_acc)
        self.__save_logs(self.epochs_logs_file,train_loss=self.e_train_loss,val_loss=self.e_val_loss,train_acc=self.e_train_acc,val_acc=self.e_val_acc)
        self.__save_models(self.models_logs_file,**self.train_parameters)
        self.RunLogs(self.logs_dir)
        return
      
    def __save_models(self,file_path, **kwargs):
        df = pd.DataFrame(kwargs, index=[0] )
        df.to_csv(file_path)
    
    def __save_logs(self,file_path, **kwargs):
        df = pd.DataFrame(kwargs)
        df.to_csv(file_path)
    
    def RunLogs(self, path, alpha=0.99):
        models_logs_path = os.path.join(path,'models_logs')
        apb = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']
        print('Logs Available:')
        print()
        
        id = 0
        mod_names = {}
        for f in os.listdir(models_logs_path):
            fp = os.path.join(models_logs_path, f)
            if os.path.isfile(fp):
                df = pd.read_csv(fp,index_col=0)
                logs = df.to_dict('list')
                stc = ''
                for key, value in logs.items():
                    stc += key + ': ' + str(value[0]) + '  '
                print('{}) run {}/{}/{} {}:{}'.format(apb[id],f[-10:-8],f[-12:-10],f[-16:-12],f[-8:-6],f[-6:-4]))
                print(stc[:-2])
                print()
                mod_names[apb[id]] = f
                id +=1
                
        while True:
            try:
                compare = input('Choose log: ')
                self.chosen_logs = []
                if compare.lower() == 'exit':
                    return
                for l in list(compare.lower()):
                    self.chosen_logs.append(mod_names[l])
                print()
                break
            except KeyError:
                print("Oops! That was no valid log(s). Try again value(s) between {} and {}, or exit".format(list(mod_names.keys())[0],list(mod_names.keys())[-1]))
                print()
                
        while True:
            temporal = input('Choose logs temporal scope between a) steps and b) epochs : ')
            if temporal.lower() == 'a' or temporal.lower() == 'b':
                print()
                break
            elif temporal.lower() == 'exit':
                return
            else:
                print("Oops! That was no valid temporal. Try again one value between a and b, or exit")
                print()
        
        if temporal == 'a':
            graphs_logs_path = os.path.join(path,'steps_logs/')
            self.x_label = 'steps'
        else:
            graphs_logs_path = os.path.join(path,'epochs_logs/')
            self.x_label = 'epochs'
            
        logs = pd.read_csv(os.path.join(graphs_logs_path,self.chosen_logs[0]),index_col=0).to_dict('list')
        
        keys_str = 'Choose measure: '
        id =0
        mes_names = {}
        for k in list(logs.keys()):
            keys_str += apb[id]+ ') '+k + ' - '
            mes_names[apb[id]] = k
            id+=1
            
        while True:
            try:
                mes_chosen = input(keys_str[:-2] + ': ')
                if mes_chosen.lower() == 'exit':
                    return
                self.chosen_measure = mes_names[mes_chosen.lower()]
                print()
                break
            except KeyError:
                print("Oops! That was no valid measure. Try again one value between {} and {}".format(list(mes_names.keys())[0],list(mes_names.keys())[-1]))
                print()
        self.chosen_data = {}
        for ch in self.chosen_logs:
            self.chosen_data[ch] = pd.read_csv(os.path.join(graphs_logs_path,ch),index_col=0)
            plt.plot(self.__ewma_vectorized(self.chosen_data[ch].to_dict('list')[self.chosen_measure],alpha=alpha), label=ch[:-4])
        plt.xlabel(self.x_label)
        plt.ylabel(self.chosen_measure)
        plt.legend(loc='best')
        plt.show()
        return
      
    def ShowLastRun(self, alpha=0.99):
        for ch in self.chosen_logs:
            plt.plot(self.__ewma_vectorized(self.chosen_data[ch].to_dict('list')[self.chosen_measure],alpha=alpha), label=ch[:-4])
        plt.xlabel(self.x_label)
        plt.ylabel(self.chosen_measure)
        plt.legend(loc='best')
        plt.show()
        
    def plot_history(self, alpha=0.99, max_y = 1.0):
        plot_loss = self.__ewma_vectorized(self.train_loss, alpha)
        plt.figure(figsize = (10, 6))
        plt.plot(np.arange(self.nb_steps), plot_loss)
        if max_y != None and min(plot_loss) < max_y: plt.ylim((0.0,max_y))
        plt.xlabel("#steps")
        plt.ylabel("train loss")
        plt.show()
        
        plot_acc = self.__ewma_vectorized(self.train_acc, alpha)
        plt.figure(figsize = (10, 6))
        plt.plot(np.arange(self.nb_steps), plot_acc)
        plt.xlabel("#steps")
        plt.ylabel("train accuracy")
        plt.show()
        
        plt.figure(figsize = (10, 6))
        plt.plot(self.e_steps, self.e_train_loss)
        plt.plot(self.e_steps, self.e_val_loss, 'g')
        if max_y != None and min(self.e_train_loss) < max_y and min(self.e_val_loss) < max_y: plt.ylim((0.0,max_y))
        plt.xlabel("#epochs")
        plt.ylabel("losses")
        plt.show()
        
        plt.figure(figsize = (10, 6))
        plt.plot(self.e_steps, self.e_train_acc)
        plt.plot(self.e_steps, self.e_val_acc, 'g')
        plt.xlabel("#epochs")
        plt.ylabel("accuracy")
        plt.show()

def split_data(data, targets, dv, cv, seed=42):
    np.random.seed(seed)
    nb_samples =  len(targets)
    random_indexes = np.random.permutation(nb_samples)
    
    X_train = data[:nb_samples-dv-cv]
    y_train = targets[:nb_samples-dv-cv]
    
    X_dev = data[nb_samples-dv-cv:nb_samples-cv] if dv != 0 else []
    y_dev = targets[nb_samples-dv-cv:nb_samples-cv] if dv != 0 else []
    
    X_cv = data[nb_samples-cv:nb_samples] if cv != 0 else []
    y_cv = targets[nb_samples-cv:nb_samples] if cv != 0 else []

    return (X_train,y_train,X_dev,y_dev,X_cv,y_cv)

def id_conv_block(x, conv_ch, lambda_, block, seed=42, trainable=True):
    #input_tensor: m x input_width x input_height x input_channel
    input_tensor = x
    
    x = layers.Conv2D(conv_ch[0],(1,1),kernel_initializer=initializers.he_normal(seed),kernel_regularizer=regularizers.l2(lambda_),trainable=trainable,name="block_{}_cv_0".format(block))(x)    
    x = layers.BatchNormalization(trainable=trainable,name="block_{}_bn_0".format(block))(x)
    x = layers.Activation('relu', name="block_{}_relu_0".format(block))(x)
    #x: m x input_width x input_height x conv_ch[0]
    
    x = layers.Conv2D(conv_ch[1],(3,3),padding='same',kernel_initializer=initializers.he_normal(seed),kernel_regularizer=regularizers.l2(lambda_),trainable=trainable,name="block_{}_cv_1".format(block))(x)
    x = layers.BatchNormalization(trainable=trainable,name="block_{}_bn_1".format(block))(x)
    x = layers.Activation('relu', name="block_{}_relu_1".format(block))(x)
    #x: m x input_width x input_height x conv_ch[1]
    
    x = layers.Conv2D(conv_ch[2],(1,1),kernel_initializer=initializers.he_normal(seed),kernel_regularizer=regularizers.l2(lambda_),trainable=trainable,name="block_{}_cv_2".format(block))(x)
    x = layers.BatchNormalization(trainable=trainable,name="block_{}_bn_2".format(block))(x)
    #x: m x input_width x input_height x conv_ch[2]
    
    x = layers.Add()([x,input_tensor])
    x = layers.Activation('relu', name="block_{}_relu_2".format(block))(x)
    
    return x

  
def conv_block(x, conv_ch, lambda_, block, strides=2, seed=42, trainable=True):
    #input_tensor: m x input_width x input_height x input_channel
    input_tensor = x

    x = layers.Conv2D(conv_ch[0],(1,1), strides=strides,kernel_initializer=initializers.he_normal(seed),kernel_regularizer=regularizers.l2(lambda_),trainable=trainable,name="block_{}_cv_0".format(block))(x)
    x = layers.BatchNormalization(trainable=trainable,name="block_{}_bn_0".format(block))(x)
    x = layers.Activation('relu', name="block_{}_relu_0".format(block))(x)
    #x: m x input_width x input_height x conv_ch[0]
    
    x = layers.Conv2D(conv_ch[1],(3,3),padding='same',kernel_initializer=initializers.he_normal(seed),kernel_regularizer=regularizers.l2(lambda_),trainable=trainable,name="block_{}_cv_1".format(block))(x)
    x = layers.BatchNormalization(trainable=trainable,name="block_{}_bn_1".format(block))(x)
    x = layers.Activation('relu', name="block_{}_relu_1".format(block))(x)
    #x: m x input_width x input_height x conv_ch[1]
    
    x = layers.Conv2D(conv_ch[2],(1,1),kernel_initializer=initializers.he_normal(seed),kernel_regularizer=regularizers.l2(lambda_),trainable=trainable,name="block_{}_cv_2".format(block))(x)
    x = layers.BatchNormalization(trainable=trainable,name="block_{}_bn_2".format(block))(x)
    #x: m x input_width x input_height x conv_ch[2]
    
    _input_tensor = layers.Conv2D(conv_ch[2],(1,1), strides=strides, kernel_initializer=initializers.he_normal(seed),kernel_regularizer=regularizers.l2(lambda_),trainable=trainable,name="block_{}_cv_skip".format(block))(input_tensor)
    _input_tensor = layers.BatchNormalization(trainable=trainable,name="block_{}_bn_skip".format(block))(_input_tensor)  
    #_input_tensor: m x input_width x input_height x conv_ch[2]
    
    x = layers.Add()([x,_input_tensor])
    x = layers.Activation('relu', name="block_{}_relu_2".format(block))(x)
    
    return x



def download_and_extract(DATA_URL,DEST_DIR,task='de',force=False):
    """
    Download and extract the STL-10 dataset
    :return: None
    """
    if not os.path.exists(DEST_DIR):
        os.makedirs(DEST_DIR)
    filename = DATA_URL.split('/')[-1]
    filepath = os.path.join(DEST_DIR, filename)
    if task == 'de' or task == 'd':
        if not os.path.exists(filepath) or force:
            def _progress(count, block_size, total_size):
             sys.stdout.write('\rDownloading %s %.2f%%' % (filename,
                 float(count * block_size) / float(total_size) * 100.0))
             sys.stdout.flush()
            filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)
            print('Downloaded', filename)
    if task == 'de' or task == 'e':
        if zipfile.is_zipfile(filepath):
            zipfile.ZipFile(filepath).extractall(DEST_DIR)
            print('Extracted')
        else:
            print('File is not a zip file')
            
def unpack_idx_rec(path_idx, path_rec,data_dir,start_label=0,start_id=0):
    imgrec = recordio.MXIndexedRecordIO(path_idx, path_rec, 'r')
    start_ = start_id
    label_ = start_label

    while True:
        header, s = recordio.unpack(imgrec.read_idx(start_))
        if header.label == label_ - 1:
            print("Found label {} at index {}".format(header.label +1, start_))
            break
        if header.label > label_ - 1:
            print("Error: start label is higher than seeked label")
            break
        start_ += 1
    
    saved_label = 0
    id_ = 1
    for i in range(start_,max(imgrec.keys)):
        header, s = recordio.unpack(imgrec.read_idx(i+1))
        img = mx.image.imdecode(s).asnumpy()
        label = header.label + 1
        sys.stdout.write('\rExtracting {:.4f}%, id: {}'.format(float(i)/float(max(imgrec.keys)) * 100.0,i))
        sys.stdout.flush()
        
        dest_directory = os.path.join(data_dir,'person_{}/'.format(int(label)))
        
        if not os.path.exists(dest_directory):
                os.makedirs(dest_directory)
            
        if label != saved_label:
            saved_label = label
            id_ = 1
            
        dest_file = os.path.join(dest_directory,'person_{}_{}.jpg'.format(int(label),int(id_)))
        imageio.imwrite(dest_file, img)
        id_ += 1
            